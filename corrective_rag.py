import os
import json
from pathlib import Path
from typing import List, Dict, Any, Literal

from dotenv import load_dotenv
from bs4 import BeautifulSoup

# --- LangChain / LangGraph imports ---
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableConfig

from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver
from typing import Annotated, List, Any
from typing_extensions import TypedDict
from langgraph.graph.message import add_messages


# ========== 0. ENV, LLM & Global Constants ==========

load_dotenv()
GEMINI_API_KEY = os.getenv("gg_api_key")

if not GEMINI_API_KEY:
    raise ValueError("GEMINI_API_KEY not found. Please set it in .env")

# Kh·ªüi t·∫°o Global LLM v√† Embedder (Hi·ªáu su·∫•t cao h∆°n)
def get_llm(model: str = "gemini-2.5-flash") -> ChatGoogleGenerativeAI:
    return ChatGoogleGenerativeAI(
        model=model,
        google_api_key=GEMINI_API_KEY,
        temperature=0.2,
    )

LLM = get_llm()
EMBEDDER = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
MAX_ATTEMPTS = 2 # Gi·ªõi h·∫°n s·ªë l·∫ßn th·ª≠ l·∫°i t·ªëi ƒëa (1 initial + 1 retry)


# ========== 1. DOCS ‚Üí VECTOR STORE (HTML SUPPORTED) ==========

DOCS_DIR = Path("docs/langgraph")
VS_DIR = "vectorstore/langgraph"

# Official URLs to attach as metadata
URL_MAP = {
    "overview": "https://docs.langchain.com/oss/python/langgraph/overview",
    "graph-api": "https://docs.langchain.com/oss/python/langgraph/graph-api",
    "workflows-agents": "https://docs.langchain.com/oss/python/langgraph/workflows-agents",
    "retrieval": "https://docs.langchain.com/oss/python/langchain/retrieval",
    "rag": "https://docs.langchain.com/oss/python/langchain/rag",
    "agents": "https://docs.langchain.com/oss/python/langchain/agents",
}

def load_docs() -> List[Document]:
    """Load docs (.html/.htm/.md/.txt), convert HTML ‚Üí text, split, add metadata."""
    if not DOCS_DIR.exists():
        raise FileNotFoundError("docs/langgraph/ not found. Please put your 6 docs there.")

    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=200)
    docs: List[Document] = []

    for f in DOCS_DIR.glob("*"):

        if f.suffix.lower() not in [".html", ".htm", ".md", ".txt"]:
            continue

        raw = f.read_text(encoding="utf-8", errors="ignore")

        if f.suffix.lower() in [".html", ".htm"]:
            soup = BeautifulSoup(raw, "html.parser")
            # C·∫£i ti·∫øn: Ch·ªâ l·∫•y text trong th·∫ª body ƒë·ªÉ lo·∫°i b·ªè header/footer kh√¥ng li√™n quan
            text = soup.body.get_text(separator="\n") if soup.body else soup.get_text(separator="\n")
        else:
            text = raw

        chunks = splitter.split_text(text)
        
        # Logic tr√≠ch xu·∫•t metadata
        first_line = text.splitlines()[0] if text.splitlines() else f.stem
        section_title = first_line.strip().lstrip("# ").strip() or f.stem
        source_url = URL_MAP.get(f.stem, f.name)

        for chunk in chunks:
            docs.append(
                Document(
                    page_content=chunk,
                    metadata={
                        "source_url": source_url,
                        "section_title": section_title,
                    },
                )
            )

    return docs


def build_vectorstore():
    """Build Chroma vector store with metadata."""
    docs = load_docs()
    print(f"Loaded {len(docs)} chunks from docs/langgraph/")

    Chroma.from_documents(
        docs,
        embedding=EMBEDDER, # S·ª¨ D·ª§NG GLOBAL EMBEDDER
        persist_directory=VS_DIR,
    )
    print(f"Vector store built at {VS_DIR}")


def get_retriever(stronger: bool = False):
    """Load Chroma store and return retriever."""
    # S·ª¨ D·ª§NG GLOBAL EMBEDDER
    vs = Chroma(
        persist_directory=VS_DIR,
        embedding_function=EMBEDDER
    )
    k = 4 if not stronger else 8
    # C√†i ƒë·∫∑t b·ªô t√¨m ki·∫øm v·ªõi k chunks
    return vs.as_retriever(search_kwargs={"k": k})


# ========== 2. RAG UTILITIES ==========

RAG_PROMPT = PromptTemplate.from_template(
    """You are a helpful assistant answering questions about LangChain and LangGraph.
Use ONLY the context below. If the context is not enough, say you are not sure.

Context:
{context}

Question:
{question}

Answer in clear English, 2‚Äì4 short paragraphs maximum.
"""
)

def format_docs(docs: List[Document]) -> str:
    return "\n\n".join(f"[{i+1}] {d.page_content}" for i, d in enumerate(docs))

def extract_citations(docs: List[Document]) -> List[Dict[str, str]]:
    # L·ªçc c√°c tr√≠ch d·∫´n ƒë·ªôc nh·∫•t (unique citations)
    unique_cits = {}
    for d in docs:
        url = d.metadata.get("source_url", "unknown")
        # S·ª≠ d·ª•ng URL l√†m kh√≥a ƒë·ªÉ ƒë·∫£m b·∫£o ch·ªâ l·∫•y 1 l·∫ßn cho m·ªói ngu·ªìn
        unique_cits[url] = {
            "source_url": url,
            "section_title": d.metadata.get("section_title", "unknown"),
        }
    return list(unique_cits.values())


def rag_answer(question: str, stronger: bool = False):
    # S·ª≠ d·ª•ng LLM v√† Retriever ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o
    retriever = get_retriever(stronger)

    docs = retriever.invoke(question)
    context = format_docs(docs)
    prompt = RAG_PROMPT.format(context=context, question=question)
    resp = LLM.invoke(prompt)

    answer_text = resp.content if isinstance(resp.content, str) else str(resp.content)
    citations = extract_citations(docs)

    return answer_text, citations


def llm_judge(question: str, answer: str, citations: List[Dict[str, str]]) -> Dict[str, Any]:
    # S·ª≠ d·ª•ng LLM ƒë√£ kh·ªüi t·∫°o
    
    # C·∫£i ti·∫øn: Y√™u c·∫ßu c√∫ ph√°p JSON m·∫°nh m·∫Ω h∆°n
    judge_prompt = f"""
You are a strict judge for a Corrective RAG system.

Given:
- User question
- Draft answer
- Citations (Sources used for the answer)

Judge the answer on its sufficiency, grounding, and relevance.
Your response MUST be a single, valid JSON object with the following schema:
{{"pass": boolean, "reasons": string, "score": integer}}
The "pass" field is TRUE only if the answer is complete, well-grounded by the citations, and directly addresses the question. Otherwise, it is FALSE.

Question:
{question}

Draft answer:
{answer}

Citations:
{json.dumps(citations, indent=2)}

Return ONLY JSON. Do not add any conversational text or markdown code blocks (e.g., ```json).
"""
    resp = LLM.invoke(judge_prompt)
    raw = resp.content.strip()

    # C·∫£i ti·∫øn: X·ª≠ l√Ω l·ªói JSON parsing (lo·∫°i b·ªè c√°c k√Ω t·ª± Markdown kh√¥ng c·∫ßn thi·∫øt)
    if raw.startswith("```json"):
        raw = raw.strip("`").strip("json").strip()
    elif raw.startswith("```"):
        raw = raw.strip("`").strip()

    try:
        verdict = json.loads(raw)
    except Exception as e:
        print(f"ERROR: Could not parse judge JSON. Fallback to FAIL. Error: {e}")
        verdict = {"pass": False, "reasons": f"Failed to parse judge output: {raw[:50]}...", "score": 1}

    # ƒê·∫£m b·∫£o c√°c tr∆∞·ªùng c·∫ßn thi·∫øt t·ªìn t·∫°i
    verdict.setdefault("pass", False)
    verdict.setdefault("reasons", "No reasons provided.")
    verdict.setdefault("score", 2)
    return verdict


def llm_rewrite(original_question: str, reasons: str) -> str:
    # S·ª≠ d·ª•ng LLM ƒë√£ kh·ªüi t·∫°o

    rewrite_prompt = f"""
Rewrite this question to be clearer and specifically address the missing information or error.
Focus on creating a better search query to retrieve relevant documents.

Original question:
{original_question}

The answer failed because of these issues:
{reasons}

Return ONLY the single, rewritten question text.
"""

    resp = LLM.invoke(rewrite_prompt)
    return resp.content.strip()


def format_final_answer(
    final_answer: str,
    final_citations: List[Dict[str, str]],
    judgements: List[Dict[str, Any]],
    query_variants: List[str],
) -> str:

    lines = []
    lines.append("### ‚úÖ Final Answer\n")
    lines.append(final_answer.strip())

    lines.append("\n\n---\n### üìö Citations\n")
    for i, c in enumerate(final_citations, 1):
        lines.append(f"{i}. **{c.get('section_title')}** ‚Äì {c.get('source_url')}")

    lines.append("\n---\n### üìù Decision Log\n")
    for i, j in enumerate(judgements, 1):
        lines.append(f"- **Attempt {i}**: {'PASS' if j['pass'] else 'FAIL'} (Score: {j['score']}/5) ‚Äì {j['reasons']}")

    if query_variants:
        lines.append("\n### üîÑ Rewritten Questions")
        for q in query_variants:
            lines.append(f"- {q}")

    return "\n".join(lines)


# ========== 3. STATE CLASS ==========

class RAGState(TypedDict):
    # T·ª± ƒë·ªãnh nghƒ©a messages thay v√¨ d√πng MessagesState
    messages: Annotated[List[Any], add_messages] 
    question: str
    draft_answers: List[str]
    citations: List[List[Dict[str, str]]]
    judgements: List[Dict[str, Any]]
    query_variants: List[str]
    attempts: int


# ========== 4. NODES ==========

def initial_rag(state: RAGState):
    """Node 1: Th·ª±c hi·ªán RAG l·∫ßn ƒë·∫ßu v√† t·∫°o c√¢u tr·∫£ l·ªùi nh√°p."""
    draft, cits = rag_answer(state["question"])
    return {
        "draft_answers": state.get("draft_answers", []) + [draft],
        "citations": state.get("citations", []) + [cits],
        "attempts": 1, # KH·ªûI T·∫†O B·ªò ƒê·∫æM
    }

def judge(state: RAGState):
    """Node 2: ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng c·ªßa c√¢u tr·∫£ l·ªùi nh√°p cu·ªëi c√πng."""
    verdict = llm_judge(
        state["question"],
        state["draft_answers"][-1],
        state["citations"][-1],
    )
    return {"judgements": state.get("judgements", []) + [verdict]}

def rewrite_query(state: RAGState):
    """Node 3: Vi·∫øt l·∫°i truy v·∫•n d·ª±a tr√™n l√Ω do th·∫•t b·∫°i."""
    # L·∫•y l√Ω do th·∫•t b·∫°i t·ª´ ph√°n quy·∫øt cu·ªëi c√πng
    reasons = state["judgements"][-1].get("reasons", "Incomplete or irrelevant answer.")
    new_q = llm_rewrite(state["question"], reasons)
    
    # C·∫≠p nh·∫≠t tr·∫°ng th√°i: ƒê·∫∑t c√¢u h·ªèi m·ªõi v√†o tr∆∞·ªùng question
    return {
        "question": new_q, 
        "query_variants": state.get("query_variants", []) + [new_q],
    }

def reretrieve_and_answer(state: RAGState):
    """Node 4: Truy xu·∫•t l·∫°i t√†i li·ªáu v·ªõi truy v·∫•n m·ªõi v√† tr·∫£ l·ªùi l·∫ßn 2."""
    # S·ª≠ d·ª•ng `stronger=True` (k=8) cho l·∫ßn truy xu·∫•t th·ª© hai
    draft, cits = rag_answer(state["question"], stronger=True)
    return {
        "draft_answers": state["draft_answers"] + [draft],
        "citations": state["citations"] + [cits],
        "attempts": state["attempts"] + 1, # TƒÇNG B·ªò ƒê·∫æM
    }

def finalize(state: RAGState):
    """Node 5: ƒê·ªãnh d·∫°ng c√¢u tr·∫£ l·ªùi cu·ªëi c√πng v·ªõi tr√≠ch d·∫´n v√† nh·∫≠t k√Ω."""
    content = format_final_answer(
        final_answer=state["draft_answers"][-1],
        final_citations=state["citations"][-1],
        judgements=state["judgements"],
        query_variants=state.get("query_variants", []),
    )
    # Th√™m c√¢u tr·∫£ l·ªùi cu·ªëi c√πng v√†o l·ªãch s·ª≠ tin nh·∫Øn
    return {
        "messages": state.get("messages", []) + [{
            "type": "ai",
            "content": content,
        }]
    }


# ========== 5. ROUTER (Conditional Edge) ==========

def route_on_judge(state: RAGState) -> Literal["rewrite", "finalize"]:
    """ƒê·ªãnh tuy·∫øn lu·ªìng d·ª±a tr√™n ph√°n quy·∫øt c·ªßa judge v√† s·ªë l·∫ßn th·ª≠."""
    
    # 1. N·∫øu judge ƒë√°nh gi√° PASS, d·ª´ng l·∫°i
    if state["judgements"][-1]["pass"]:
        print("DEBUG: Judge PASS. Finalizing answer.")
        return "finalize"
    
    # 2. N·∫øu ƒë√£ ƒë·∫°t gi·ªõi h·∫°n l·∫ßn th·ª≠, d·ª´ng l·∫°i ƒë·ªÉ tr√°nh v√≤ng l·∫∑p v√¥ t·∫≠n
    if state["attempts"] >= MAX_ATTEMPTS:
        print(f"DEBUG: Max attempts ({MAX_ATTEMPTS}) reached. Finalizing current answer.")
        return "finalize"
    
    # 3. N·∫øu th·∫•t b·∫°i v√† ch∆∞a ƒë·∫°t gi·ªõi h·∫°n, th·ª≠ l·∫°i
    print("DEBUG: Judge FAIL. Rewriting query and retrying.")
    return "rewrite"


# ========== 6. BUILD GRAPH APP ==========

def build_app():
    """X√¢y d·ª±ng v√† bi√™n d·ªãch LangGraph."""
    graph = StateGraph(RAGState)

    # 1. Th√™m c√°c Node x·ª≠ l√Ω
    graph.add_node("initial_rag", initial_rag)
    graph.add_node("judge", judge)
    graph.add_node("rewrite", rewrite_query)
    graph.add_node("reretrieve_and_answer", reretrieve_and_answer)
    graph.add_node("finalize", finalize)

    # 2. Thi·∫øt l·∫≠p Edges (ƒê∆∞·ªùng ƒëi c·ªë ƒë·ªãnh)
    graph.add_edge(START, "initial_rag")
    graph.add_edge("initial_rag", "judge")
    graph.add_edge("rewrite", "reretrieve_and_answer")
    graph.add_edge("reretrieve_and_answer", "judge") # Quay l·∫°i judge sau khi th·ª≠ l·∫°i

    # 3. Thi·∫øt l·∫≠p Conditional Edge (ƒê∆∞·ªùng ƒëi c√≥ ƒëi·ªÅu ki·ªán)
    graph.add_conditional_edges(
        "judge",
        route_on_judge,
        {
            "rewrite": "rewrite",
            "finalize": "finalize",
        }
    )

    graph.add_edge("finalize", END)

    # Bi√™n d·ªãch ƒë·ªì th·ªã v√† th√™m Checkpointer (MemorySaver)
    return graph.compile(checkpointer=MemorySaver())


# ========== 7. DEMO RUN ==========

if __name__ == "__main__":

    # Ki·ªÉm tra v√† x√¢y d·ª±ng Vector Store n·∫øu ch∆∞a t·ªìn t·∫°i
    if not Path(VS_DIR).exists():
        print("Vector store not found. Building...")
        build_vectorstore()
    else:
        print("Vector store exists. Skipping build.")

    app = build_app()

    print("\n=== Corrective RAG Demo ===")
    user_question = input("Enter your question: ")

    # Chu·∫©n b·ªã tr·∫°ng th√°i kh·ªüi t·∫°o
    init_state = {
        "messages": [{"type": "human", "content": user_question}],
        "question": user_question,
        "draft_answers": [],
        "citations": [],
        "judgements": [],
        "query_variants": [],
        "attempts": 0, # Kh·ªüi t·∫°o l√† 0
    }

    # C·∫•u h√¨nh thread ID ƒë·ªÉ s·ª≠ d·ª•ng MemorySaver
    config = RunnableConfig(configurable={"thread_id": "demo-thread"})
    final = app.invoke(init_state, config=config)

    print("\n========== FINAL OUTPUT ==========\n")
    
    last_msg = final["messages"][-1]
    
    # Ki·ªÉm tra xem n√≥ l√† Object hay Dict ƒë·ªÉ x·ª≠ l√Ω ph√π h·ª£p
    if hasattr(last_msg, "content"):
        print(last_msg.content)
    else:
        print(last_msg.get("content"))
        
    print("\n=================================\n")