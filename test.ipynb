{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e55b28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for langchain, Chroma, and embeddings\n",
    "%pip install -q langchain langchain-community langchain-openai langchain-core chromadb sentence-transformers openai\n",
    "%pip install google-generativeai langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba0768f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableMap, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb8fa39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gg_api_key=\"AIzaSyDDFxNLEEIRDksup7lKT7o8c0NoJDN1lS4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382fb454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI learns from data to find patterns and make decisions or predictions.\n"
     ]
    }
   ],
   "source": [
    "# Test API Key from Google\n",
    "\n",
    "from google import genai\n",
    "\n",
    "# The client gets the API key from the environment variable `GEMINI_API_KEY`.\n",
    "client = genai.Client(api_key=gg_api_key)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\", contents=\"Explain how AI works in a few words\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b524ef0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am sorry, but the provided context does not contain information about what LangGraph state is. The context only states, \"This context is about LangGraph.\"\n"
     ]
    }
   ],
   "source": [
    "# ---- Embeddings + Vector DB ----\n",
    "embedder = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "# Lưu ý: \"docs\" phải là thư mục chứa Chroma DB đã được khởi tạo\n",
    "vectordb = Chroma(persist_directory=\"docs\", embedding_function=embedder)\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# ---- LLM (Gemini) ----\n",
    "# Đảm bảo gg_api_key được truyền vào nếu không dùng biến môi trường\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    google_api_key=gg_api_key\n",
    ")\n",
    "\n",
    "# ---- Convert docs → string (Được giữ nguyên) ----\n",
    "def format_docs(docs):\n",
    "    # Trả về chuỗi chứa nội dung của các tài liệu được truy xuất\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "# ---- Prompt (Được giữ nguyên) ----\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "Use ONLY the context below to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\")\n",
    "\n",
    "# ---- Chain (SỬA LỖI VÀ TỐI ƯU HÓA) ----\n",
    "# Sử dụng StrOutputParser để trích xuất nội dung văn bản từ đối tượng AIMessage\n",
    "rag_chain = (\n",
    "    RunnableMap({\n",
    "        # 1. Truy xuất tài liệu và định dạng thành chuỗi context\n",
    "        \"context\": retriever | format_docs, \n",
    "        # 2. Truyền câu hỏi của người dùng nguyên vẹn\n",
    "        \"question\": RunnablePassthrough(), # Tối ưu hóa: Thay thế lambda x: x[\"question\"]\n",
    "    })\n",
    "    | prompt # 3. Đưa context và question vào PromptTemplate\n",
    "    | llm    # 4. Truyền prompt đã định dạng cho LLM\n",
    "    | StrOutputParser() # 5. Lấy nội dung text thuần túy từ kết quả của LLM\n",
    ")\n",
    "\n",
    "# ---- Run ----\n",
    "# LƯU Ý: Nếu bạn sử dụng RunnablePassthrough, bạn chỉ cần truyền chuỗi câu hỏi.\n",
    "# Nếu giữ nguyên RunnableMap, bạn phải truyền dict: {\"question\": \"...\"}\n",
    "res = rag_chain.invoke(\"What is LangGraph state?\") \n",
    "\n",
    "# In kết quả. res đã là chuỗi text thuần túy nhờ StrOutputParser\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
